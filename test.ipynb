{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hooks for direct importing from notebooks.\n",
    "import Utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from RL_brain import DeepQNetwork\n",
    "import numpy as np\n",
    "from env import Env\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import printPeriodic,setDbgPrint,null\n",
    "from data_Generator import data_Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = {\n",
    "    'cfg' : {\n",
    "        'run' : { # run parameters\n",
    "            'numEpisodes'    : 1, # 1,        # num. of episodes to run\n",
    "            'maxSteps'       : 100,         # max. num. of sim. steps\n",
    "            'startLearnStep' : 200,           # start periodic training after this step num.\n",
    "            'learnPeriod'    : 5,             # num. of steps between (re)training\n",
    "            'dbgPrint'       : printPeriodic, # function or string name of function for debug print output\n",
    "            'statusPeriod'   : 100,           # output program status at this interval\n",
    "        },\n",
    "        'dqn': {  # DQN parameters\n",
    "            'learningRate': 0.01,  # please provide summary comments for each parameter\n",
    "            'rewardDecay': 0.9,\n",
    "            'eGreedy': 1,\n",
    "            'eGreedyincrement': 0.005,  # Change the e_greedy value\n",
    "            'replaceTargetIter': 200,\n",
    "            'memorySize': 2000,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class run():\n",
    "    def __init__(self ):\n",
    "        self.actions = [\n",
    "            \"Channel_1\",\n",
    "            \"Channel_6\",\n",
    "            \"Channel_11\"\n",
    "        ]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_features = 3\n",
    "        self.built_net()\n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.saver.restore(self.sess, 'my_net/my_test_model.ckpt') #Load parameter\n",
    "        self.params = []\n",
    "        self.cost_his = []\n",
    "        self.time = 0\n",
    "        self.time_env_state = {}\n",
    "        self.num = 0\n",
    "        self.list = data_Generator()\n",
    "        self.reward_history = []\n",
    "    def built_net(self):\n",
    "        # Reconstruction neural network model.Only one of the neural networks is kept here.\n",
    "        c_names, n_l1, w_initializer, b_initializer = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \\\n",
    "                                                      tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)\n",
    "        self.env_state_ = tf.placeholder(tf.float32, [None, self.n_features], name='env_state_')\n",
    "        with tf.variable_scope('target_net'):\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            with tf.variable_scope(\n",
    "                    'l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.tanh(tf.matmul(self.env_state_, w1) + b1)\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "\n",
    "    def choose_action(self, env_state):\n",
    "        env_state = env_state[np.newaxis, :]\n",
    "        actions_value = self.sess.run(self.q_next,feed_dict={self.env_state_: env_state})\n",
    "        action = np.argmax(actions_value)\n",
    "        return action\n",
    "    def data(self):\n",
    "\n",
    "        channel = self.list[self.num]\n",
    "        self.num +=1\n",
    "        return channel\n",
    "\n",
    "    def reset(self):\n",
    "        self.update_State()\n",
    "        # self.state = \"Channel_1\"\n",
    "        return self.time_env_state[\"current\"]\n",
    "\n",
    "\n",
    "    def update_State(self):\n",
    "        self.time += 1\n",
    "        filename = 'data.csv'\n",
    "\n",
    "        self.time_env_state[\"current\"] = {\"Channel_1\": self.data(),\n",
    "                                          \"Channel_6\": self.data(),\n",
    "                                          \"Channel_11\": self.data(), }\n",
    "\n",
    "        return self.time_env_state[\"current\"]\n",
    "    def step(self,action):\n",
    "        value = 0\n",
    "        min_value = np.Inf\n",
    "        action_key = \"\"\n",
    "\n",
    "       # self.log('{}: action {} has min. value {}\\n'.format(self.time, action_key, min_value), period=self.statusPeriod,\n",
    "                # counter=self.time)\n",
    "\n",
    "\n",
    "        for key in self.time_env_state[\"current\"]:\n",
    "\n",
    "            value = self.time_env_state[\"current\"][key][0]\n",
    "            if value < min_value:\n",
    "                min_value = value\n",
    "                action_key = key\n",
    "        #reward = self.reward(action)\n",
    "        if action_key == action:\n",
    "            self.reward = 20\n",
    "        else:\n",
    "            self.reward = 0\n",
    "        self.reward_history.append(self.reward)\n",
    "        # self.correct_rate.append(self.count/(len(self.count_history)))\n",
    "        next_state = action\n",
    "        self.state = next_state\n",
    "        self.update_State()\n",
    "\n",
    "        return self.time_env_state[\"current\"], self.time_env_state[\"current\"][next_state]\n",
    "    def run_(self,cfg=None):\n",
    "        numEpisodes = cfg['numEpisodes']\n",
    "        maxSteps = cfg['maxSteps']\n",
    "        dbgPrint = cfg.get('dbgPrint', null)\n",
    "\n",
    "        statusPeriod = cfg.get('statusPeriod', 1)\n",
    "        for episode in range(1,numEpisodes+1):\n",
    "            # initial observation\n",
    "            step = 0\n",
    "            env_state_1 = self.reset()\n",
    "            while step < maxSteps:\n",
    "                print('{}: current env = {}\\n'.format(self.time, self.time_env_state))\n",
    "                env_state = np.hstack((env_state_1[\"Channel_1\"], env_state_1[\"Channel_6\"],\n",
    "                                       env_state_1[\"Channel_11\"]))\n",
    "\n",
    "                action = self.choose_action(env_state)\n",
    "                if action == 0:\n",
    "                    action_ = \"Channel_1\"\n",
    "                elif action == 1:\n",
    "                    action_ = \"Channel_6\"\n",
    "                else:\n",
    "                    action_ = \"Channel_11\"\n",
    "                observation = env_state_1[action_]\n",
    "                dbgPrint('{}: action_ = {}, observation = {}\\n'.format(self.time, action_, observation),\n",
    "                         period=statusPeriod, counter=self.time)\n",
    "                print('{}: action_ = {}, observation = {}\\n'.format(self.time, action_, observation))\n",
    "                env_state_, observation_ = self.step(action_)\n",
    "                env_state_ = np.hstack(( env_state_[\"Channel_1\"],  env_state_[\"Channel_6\"],\n",
    "                                env_state_[\"Channel_11\"]))\n",
    "                env_state_ = {\"Channel_1\": env_state_[0:1],\n",
    "                              \"Channel_6\": env_state_[1:2],\n",
    "                              \"Channel_11\": env_state_[2:3]\n",
    "\n",
    "                              }\n",
    "                env_state_1 = env_state_\n",
    "                step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    statusPeriod = sim['cfg']['run']['statusPeriod']\n",
    "    dbgPrint = setDbgPrint(sim['cfg']['run'].get('dbgPrint'))\n",
    "    env = Env(log=dbgPrint, statusPeriod= statusPeriod)\n",
    "    run = run()\n",
    "    run.run_(cfg=sim['cfg']['run'])\n",
    "    plt.subplot(1,2 ,1 )\n",
    "    N = 100\n",
    "    specHi = 50 + 50 * np.sin(2 * np.pi * np.arange(N) / N)\n",
    "    spec1 = np.array([50] * N)\n",
    "    spec2 = 50 + 50 * np.cos(2 * np.pi * np.arange(N) / N)\n",
    "    plt.plot(np.c_[specHi, spec1, spec2], '.-')\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Spectral density')\n",
    "    plt.legend(['channel 1', 'channel 6', 'channel 11'])\n",
    "    plt.title('Sample Time-Varying Spectral Density')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(run.reward_history, '.-')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Cumulative % selection action is min. value action')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
